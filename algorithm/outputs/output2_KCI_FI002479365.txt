본문 요약 (10줄)
원문 : This explosion of dimension also causescomputational problems regarding memory size and speed of processing.avoid the explosion of dimension, we use the limited word set in forming n-gramsby limiting the part-of-speech tag of words to nouns (NNG), adjectives (VA, VAX),classification and to avoid multiple counting, we consider only the highest n-gramwhen multiple overlapping n-grams are found in each sentence
번역 : 이러한 차원의 폭발은 또한 메모리 크기 및 처리 속도와 관련된 계산 문제를 유발합니다. 차원의 폭발을 피하고, 단어의 품사 태그를 명사 (NNG), 형용사 ()로 제한하여 n- 그램을 형성하는 데 제한된 단어 집합을 사용합니다. VA, VAX), 분류 및 다중 계수를 피하기 위해 각 문장에서 여러 개의 겹치는 n-gram이 발견 될 때 가장 높은 n-gram 만 고려합니다.

원문 : The other isthe corpus-based approach that classifies polarity by using word (n-gram in our case) because it assumes that all features are independent given the class (hawkish or dovish in our case)
번역 : 다른 하나는 단어 (우리의 경우 n-gram)를 사용하여 극성을 분류하는 말뭉치 기반 접근 방식입니다. 이는 클래스 (우리의 경우 매파 적 또는 비둘기파 적)가 주어지면 모든 기능이 독립적이라고 가정하기 때문입니다.

원문 : The trainedNBC yields the conditional probability of each feature given the class (hawkish/dovish), which we use as a polarity score of the feature:An n-gram is roughly labeled as “hawkish” if it presents itself more often inGiven that we use random sampling and a probabilistic classifier, every trainingyields different posterior probability of each class
번역 : 훈련 된 NBC는 클래스 (매파 / 도비시)에 주어진 각 특성의 조건부 확률을 산출하며,이를 특성의 극성 점수로 사용합니다. 랜덤 샘플링과 확률 적 분류기, 모든 훈련은 각 클래스의 다른 사후 확률을 산출합니다.

원문 : Theparameters we use for training are 5-grams for center words, 5-grams for contextwords, window size of 5, negative sampling size of 5, and 300 dimensions for vector frequency limit of 25, which yield 410,902,512 pairs of n-grams (21.7 GB in size).With this resulting n-gram vector, we bootstrap by running our propagation 50times over 10 random equally sized subsets of the hawkish and dovish seed sets.Table 5 shows the seed sets.Similar to the market approach, we classify the polarity of our lexicons ashawkish (dovish) if the polarity score is greater (less) than 1, excluding lexicon inthe gray area by using intensity of 1.1 as a threshold
번역 : 훈련에 사용하는 매개 변수는 중앙 단어에 5 그램, 컨텍스트 단어에 5 그램, 창 크기 5, 음수 샘플링 크기 5, 벡터 주파수 제한 25에 대해 300 차원으로, 410,902,512 쌍의 n- 그램 (21.7 이 결과로 생성 된 n-gram 벡터를 사용하여 매파 및 도비시 시드 세트의 동일한 크기의 무작위 하위 집합 10 개에 대해 전파를 50 배 실행하여 부트 스트랩합니다. 표 5는 시드 세트를 보여줍니다. 시장 접근 방식과 유사하게 1.1의 강도를 임계 값으로 사용하여 회색 영역의 어휘집을 제외하고 극성 점수가 1보다 크면 (더 적은) 우리 어휘의 극성이 매파 적 (도비시)입니다.

원문 : Among them, 9,791 n-grams (69% of common n-grams) have the same the performance: Accuracy, Recall, Precision, and F1 score, which we will explain.Finally, we compare the performance of our lexicons with Korean SentimentFirst, we compare the performance of our indicators by using the documents notused in building our lexicons
번역 : 그중 9,791 개의 n-gram (일반적인 n-gram의 69 %)은 정확도, 재현율, 정밀도, F1 점수 등 동일한 성능을 가지고 있으며, 마지막으로 우리 어휘의 성능을 Korean SentimentFirst와 비교합니다. 어휘집 작성에 사용되지 않은 문서를 사용하여 지표의 성과를 비교합니다.

원문 : The correlation coefficient between thetonetonetone with those of the BOK Figure 5 compares our indicator, CPI (Picault and Renault (2017), to account for the BOK’s non-standard monetary policy, for a hawkish monetary policy decision (an increase of policy rate by 25 basis points),industrial production and its trend from Hodrick-Prescott filter
번역 : thetonetonetone과 한은의 상관 계수 그림 5는 매파 적 통화 정책 결정 (정책 금리 25 % 인상)에 대한 한은의 비표준 통화 정책을 설명하기 위해 우리의 지표 인 CPI (Picault and Renault (2017))를 비교합니다. 기준점), 산업 생산 및 Hodrick-Prescott 필터의 추세

원문 : We formally test the explanatory power of variousindicators in an augmented Taylor rule specification.withtonetMPD , outputtone To assess the relation between the BOK MPB’s policy rate decisions and theinformation content of MPB minutes, we test the explanatory power of our lexicon-based indicators (We basically consider three kinds of specifications: a typical specification expressedin level and two kinds of differenced forms used in Picault and Renault (2017) andApel and Grimaldi (2014).andtoneFirst, we consider the following specification:wheregap defined as the difference between CPI and the inflation target (set to 3% before 2016 and 2% thereafter
번역 : 강화 된 Taylor 규칙 사양에서 다양한 지표의 설명력을 공식적으로 테스트합니다 .withtonetMPD, outputtone 한은 MPB의 정책 비율 결정과 MPB 분의 정보 내용 간의 관계를 평가하기 위해 어휘 기반 지표의 설명력을 테스트합니다 (기본적으로 세 종류의 사양을 고려합니다 : 수준으로 표현 된 일반적인 사양과 Picault와 Renault (2017) 및 Apel과 Grimaldi (2014)에서 사용되는 두 가지 다른 형식 및 tone 먼저 다음 사양을 고려합니다. 목표 (2016 년 이전 3 %, 이후 2 %로 설정)

원문 : Considering the possibility that the BOK may refer to the movement ofexchange rate for its monetary policy decision, we use the rolling standard deviation for the past twelve months.information even after accountingTable 7 shows the result of OLS estimation of (8) with robust standard errorsindicators provide amplegaps are statistically significant, the explanatory power of inflation gap vanishes aswe include the lagged dependent variable in Column (2)
번역 : 한은이 통화 정책 결정시 환율 변동을 참조 할 가능성을 고려하여 지난 12 개월 동안의 롤링 표준 편차를 사용하였으며, 회계 후에도 정보 표 7은 (8)의 OLS 추정 결과와 강력한 표준 오차 지표를 보여주고있다. 앰플 갭이 통계적으로 유의미한 경우 열 (2)에 지연된 종속 변수를 포함하면 인플레이션 갭의 설명력이 사라집니다.

원문 : When the dependent variable is the currentchange in the BOK policy rate (fails to capture the change in BOK policy rate although it is based on the mosttonetoneisis based on thetonetonewithhas considerably less variations thanseems rather weak
번역 : 종속 변수가 현재 한은 정책 금리의 변화 일 때 (한은은 정책 금리의 변화를 포착하지 못하지만, 그 변화는 다소 약한 것보다 상당히 적은 변동이있는

원문 : If we interpret theBOK policy rate as a threshold or latent variable of our indicator of monetary policysentiment, feeding our measure into the standard VAR systems or DSGE modelsthat analyze the effect of monetary policy would be interesting
번역 : 한은 정책 금리를 통화 정책 감정 지표의 임계치 또는 잠복 변수로 해석하면 통화 정책의 효과를 분석하는 표준 VAR 시스템 또는 DSGE 모델에 우리의 조치를 적용하는 것이 흥미로울 것입니다.


