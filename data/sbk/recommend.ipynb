{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from parse_kci import load_dataframes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2vec = KeyedVectors.load_word2vec_format(\\\n",
    "        'F:/Install/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=900000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('queen', 0.7118192911148071),\n ('monarch', 0.6189674139022827),\n ('princess', 0.5902431607246399)]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "w2vec.most_similar(positive=['king', 'woman'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5648359"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "w2vec.similarity('korea', 'japan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Software', 0.48984354734420776),\n ('iLife_iWork', 0.47542810440063477),\n ('programmer', 0.4731772243976593),\n ('Qlusters', 0.4657154083251953),\n ('Pramati', 0.46331697702407837)]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "w2vec.most_similar(positive=['chef'], topn=5)\n",
    "w2vec.most_similar(positive=['chef', 'software'], negative=['food'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "DATA_DIR = \".\"\n",
    "DUMP_FILE = os.path.join(DATA_DIR, \"abstract_en.pkl\")\n",
    "\n",
    "data = pd.read_pickle(DUMP_FILE)\n",
    "#papers = data.drop_duplicates([\"title_en\"]).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def remove_stop_words(corpus):\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    # print(stop_words)\n",
    "    \n",
    "    #word_tokens = word_tokenize(corpus)\n",
    "    # stop_words = ['is', 'a', 'will', 'be', 'the', 'object', 'this', 'study', 'was', 'very', 'The', 'to', 'of', 'This', 'Study', 'in', 'role', 'examine', 'by', 'concluded', 'can', 'both', 'that', 'due', 'could']\n",
    "    \n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        result = []\n",
    "        if type(text)==type(1.0): continue\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        sent_tokenized = sent_tokenize(text)\n",
    "        tmp = re.sub('\\W*\\b\\w{1,2}\\b', '', sent_tokenize)\n",
    "        # shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "        \n",
    "        \n",
    "        for stop_word in stop_words:\n",
    "            if stop_word in tmp:\n",
    "                tmp.remove(stop_word)\n",
    "        result = [word_tokenize(sentence) for sentence in tmp]\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-fcf73bdbf386>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-114-7ee09f6ab872>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^a-zA-Z]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msent_tokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\W*\\\\b\\w{1,2}\\\\b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "corpus = remove_stop_words(data[\"abstract\"])\n",
    "print(len(corpus))\n",
    "\n",
    "for line in corpus[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-173a2f882055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mf:\\SSAFY\\BigDataPJT\\SubPJT1\\s03p22a406\\venv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32mf:\\SSAFY\\BigDataPJT\\SubPJT1\\s03p22a406\\venv\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m             self.train(\n\u001b[0;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\SSAFY\\BigDataPJT\\SubPJT1\\s03p22a406\\venv\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    920\u001b[0m         \"\"\"\n\u001b[0;32m    921\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[1;32m--> 922\u001b[1;33m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[0;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\SSAFY\\BigDataPJT\\SubPJT1\\s03p22a406\\venv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m   1401\u001b[0m             \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         logger.info(\n",
      "\u001b[1;32mf:\\SSAFY\\BigDataPJT\\SubPJT1\\s03p22a406\\venv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1385\u001b[0m                 )\n\u001b[0;32m   1386\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1387\u001b[1;33m                 \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1388\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# data generation\n",
    "\n",
    "# word2int = {}\n",
    "\n",
    "# for i,word in enumerate(words):\n",
    "#     word2int[word] = i\n",
    "\n",
    "# sentences = []\n",
    "# for sentence in corpus:\n",
    "#     sentences.append(sentence.split())\n",
    "    \n",
    "# WINDOW_SIZE = 2\n",
    "\n",
    "# data = []\n",
    "# for sentence in sentences:\n",
    "#     for idx, word in enumerate(sentence):\n",
    "#         for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "#             if neighbor != word:\n",
    "#                 data.append([word, neighbor])\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=corpus, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "es moderates the impact of technology anxiety on perceived productivity and attitude  for the purpose of the study  a web based survey with korean consumers was conducted  the final sample size was      structural equation modeling analysis and process in spss were employed to test the proposed hypotheses  the findings indicated that technology anxiety negatively affected perceived productivity and attitude toward self service technologies in which perceived productivity affected attitude positively  need for interaction with employees was found to moderate the relationship between technology anxiety and perceived productivity  it also moderated the relationship between technology anxiety and attitude  this study contributes to the self service technology literature by identifying two antecedents of consumer attitude toward self  service technologies  technology anxiety and the need for interaction  the findings further provide valuable insights to retailers and marketers as to how technology anxiety  perceived productivity  and the need for interaction work in enhancing consumer attitude toward self service technologies in the context of fashion retail\nmore than        fires occur nationwide and cause over      casualties every year  there is a lack of specialized equipment  and rescue operations are conducted with a minimal number of apparatuses  through the wall radars  ttwrs  can improve the rescue efficiency  particularly under limited visibility due to smoke  walls  and collapsed debris  to overcome detection challenges and maintain a small form factor  a ttwr system on chip  soc  and its architecture have been proposed  additive reception based on coherent clocks and reconfigurability can fulfill the ttwr demands  a clock based single chip infrared radar transceiver with embedded control logic is implemented using a     nm complementary metal oxide semiconductor  clock signals drive the radar operation  signal to noise ratio enhancements are achieved using the repetitive coherent clock schemes  the hand held prototype radar that uses the ttwr soc operates in real time  allowing seamless data capture  processing  and display of the target information  the prototype is tested under various pseudo disaster conditions  the test standards and methods  developed along with the system  are also presented\ncurrent assumptions are used in the formulation of pseudo first  pfo  and second order  pso  models to describe the kinetic data of filtration based on ideal operating conditions  this paper presents a new model developed with pseudo nth order and based on real assumption  a comparison was performed between pfo  pso and the new model to highlight their performance and the optimisation of the pseudo order equation  using matlab software  adsorption characteristic of bovine serum albumin adsorption on the track etched membrane are used as a medium based on protein filtration data were extracted from the literature for different concentrations to demonstrate the comparison between pfo pso and the new model  the pseudo first and second order kinetic models were applied to test the experimental data and they did not provide reasonable values  the results show that the predicted values are consistent with experimental values giving a good correlation coefficient r          and a minimum root mean squared error rmse           indeed  the experimental results follow the new model and the optimal pseudo equation order n          the most suitable curves for the new model  as a result  we used different experimental adsorption data from the literature to examine and check the applicability and validity of the model\nceramic zns nanocomposites were prepared by mechanical processing and one step heat sintering with powder mixtures offly ash  waste glass  and zns  template free hydrothermal method manufacturing   chemical durability and morphologicalcharacteristics of heat treated samples at     oc with without acid treatment were evaluated  the photocatalytic activities wereestimated with methyl orange  mo   methylene blue  mb   acetaldehyde  ata   and     dichlorophenoxyacetic acid      d  asphotodegradation targets  crystallization behaviors of the prepared ceramic zns nanocomposites were investigated using xraydiffraction  xrd   field emission scanning electron microscopy  fe sem   and energy dispersive x ray spectrometry eds   in addition  compressive and bending strength as mechanical properties were evaluated  ceramic zns nanocompositeswere found to showed improvement in optimal photocatalytic reaction and physical properties regardless of acid treatmentwhen the amount of zns nanoparticles was increased from     to      wt   degrees of photocatalytic decomposition of mo ata      d  and mb by acid treated ceramic zns nanocomposites containing    wt  zns were about                            respectively  after uv irradiation for     min\nfinite element  fe  model based structural damage detection  sdd  methods play vital roles in effectively locating and quantifying structural damages  among these methods  structural model updating should be conducted before sdd to obtain benchmark models of real structures  however  the characteristics of updating parameters are not reasonably considered in existing studies  inspired by the l  norm regularization  a novel anti sparse representation method is proposed for structural model updating in this study  based on sensitivity analysis  both frequencies and mode shapes are used to define an objective function at first  then  by adding l  norm penalty  an optimization problem is established for structural model updating  as a result  the optimization problem can be solved by the fast iterative shrinkage thresholding algorithm  fista   moreover  comparative studies with classical regularization strategy  i e  the l  norm regularization method  are conducted as well  to intuitively illustrate the effectiveness of the proposed method  a   dof spring mass model is taken as an example in numerical simulations  the updating results show that the proposed method has a good robustness to measurement noises  finally  to further verify the applicability of the proposed method  a six storey aluminum alloy frame is designed and fabricated in laboratory  the added mass on each storey is taken as updating parameter  the updating results provide a good agreement with the true values  which indicates that the proposed method can effectively update the model parameters with a high accuracy\nbackground  insufficient vascularization hampers bone tissue engineering strategies for reconstructing large bone defects  delivery of prolyl hydroxylase inhibitors  phis  is an interesting approach to upregulate vascular endothelial growth factor  vegf  by mimicking hypoxic stabilization of hypoxia inducible factor  alpha  hif      this study assessed two phis  dimethyloxalylglycine  dmog  and baicalein for their effects on human adipose tissue derived mesenchymal stem stromal cells  at mscs     methods  isolated at mscs were characterized and treated with phis to assess the cellular proliferation response  immunostaining and western blots served to verify the hif    stabilization response  the optimized concentrations for long term treatment were tested for their effects on the cell cycle  apoptosis  cytokine secretion  and osteogenic differentiation of at mscs  gene expression levels were evaluated for alkaline phosphatase  alpl   bone morphogenetic protein    bmp    runt related transcription factor    runx    vascular endothelial growth factor a  vegfa   secreted phosphoprotein    spp    and collagen type i alpha    col a    in addition  stemness related genes kruppel like factor    klf    nanog homeobox  nanog   and octamer binding transcription factor    oct   were assessed    results  phis stabilized hif    in a dose dependent manner and showed evident dose  and time dependent antiproliferative effects  with doses maintaining proliferation  dmog and baicalein diminished the effect of osteogenic induction on the expression of runx   alpl  and col a   and suppressed the formation of mineralized matrix  suppressed osteogenic response of at mscs was accompanied by an upregulation of stemness related genes    conclusion  phis significantly reduced the osteogenic differentiation of at mscs and rather upregulated stemness related genes  phis proangiogenic potential should be weighed against their longterm direct inhibitory effects on the osteogenic differentiation of at mscs\nthis study is to establish the items on class critiques based on the mathematical competencies according to the mathematics curriculum revised in       namely  this study deals with the items on how pre service or in service teachers understand and comment on mathematics instruction on their own instruction or peers  instruction  to accomplish this  first of all the draft items on instructional reviews was developed by researchers of this study on the basis of the previous study hwang         in order to revise and develop the draft items  the experimental study was executed  the experimental study was done by the subject of    groups who are undergraduate students in the educational college of c university  the subject was supposed to watch an in service middle school mathematics teacher s excellent instruction study  video and to comment the instruction video on the draft items on class critiques  while analyzing the comments of the subject  the revised items on the class critiques were to be develop  based on this study  from now on  the final and ideal items on the class critiques would be establish to reflect and comment teachers  instruction\nundoubtedly  logs are brain of any software system  development  debugging and upgradation of software applications became much easier due to the logging or auditing concept in computer science field  virtual machines also log and timestamp every activity that take place during their runtime  gathering system details from those lengthy log files is a hectic work for the end user since single log file contains millennial entries of audited log data  a management system can be very helpful solution and required for the analytical scanning and the visualization of the logged data and thus providing the way to end user for the relevant information about the virtual machines running on the hypervisors  the same system can help developers to gather relevant log entries and presenting data collectively to the end user  this system not only helps in providing and presenting details but also act as an alert system to notify user fatal errors occurred during runtime  statistical usage information or notification system can further be developed by the means of this syslog system  this paper will present how developers can build an efficient syslog management system on the web using the  cgi  common gateway interface in the c programming language and also mentions how cgi environment can be achieved in the apache tomcat webserver to build dynamic web tools  also  it provides basic idea how c can be used effectively in the cgi interface to provide better methods for obtaining and extracting relevant system data\nbackground and objective  horticultural activity is one of the most basic elements of horticultural therapy  which brings about therapeutic effects for participants through various plant related activities  the main objective of this study was to verify the results of previous research  which suggested six types of activities from the exploratory factor analysis  methods  to meet the purpose of this study  a questionnaire was designed to determine the preferences for   types of the horticultural therapy activities  the survey was conducted on     people from march   to june           the data of     cases were used into the final analysis  excluding unreliable responses  descriptive statistics  and reliability analysis were performed using ibm spss statistics     and confirmatory factor analysis was performed using ibm spss amos     results  first  horticultural therapy activities were classified into   types from the exploratory factor analysis  as conducted in previous research  the confirmatory factor analysis provided that the fit of the final model was satisfactory with                 p          rmr         gfi         rmsea         nfi         tli         cfi         conclusion  this result revalidated that the mode with   types of horticultural therapy activities from previous research is appropriate criteria for the classification of horticultural activities  the model could be used to design more systematic horticultural therapy programs that meet the needs or circumstances of the subject  or that are suitable for necessary therapeutic intervention methods\nrural areas are undergoing rapid population reduction and aging  which have caused various issues  recently  smart businesses  such as regional revitalization policies that incorporate ict and expansion of the  th industry  have been attracting attention as alternatives for solving rural problems  but few r d policies are available that are concerned with the safety of farmers  therefore  in order to advance a safe agricultural environment  it is necessary to develop a smart rural safety model that is suitable for the characteristics and prevention of safety related disasters in rural areas  in this study  the current status of smart village promotion at home and abroad was looked into  and the safety issues faced by rural communities were analyzed  based on which directions the ict based smart rural safety model has proposed  this study is expected to be used as basic data for determining r d directions for smart safety smodel specialized in rural areas and may also help establish development strategies related to smart rural safety\nspent coffee grounds  scg   the residue after brewing coffee beverage  is a promising biodiesel feedstock due to its high oil contents           however  scg should be pretreated to reduce the high free fatty acid content  which hampers transesterification reaction  to overcome this  we explored a direct transesterification reaction of scg using ultrasound irradiation and identified the optimal sonication parameters  a high fatty acid methyl ester  fame  content  up to        could be achieved with ultrasound amplitude of       m  irradiation time of    min  and methanol to oil ratio of     in the presence of potassium hydroxide concentration of      wt    in addition  we demonstrated that ultrasound irradiation is an efficient method to produce biodiesel from untreated scg in a short time with less energy than the conventional mechanical stirring method  the physical and chemical properties of the scg biodiesel met the requirements for an alternative fuel to the current commercial biodiesel\nthe study investigated the effects of sns usage on   s and   s female sns users  internalization of thin body  body surveillance  appearance upper comparison  body dissatisfaction and eating disorders based on objectification theory and social comparison theory  the study examined differences on sns usage and body image variables between high and low groups of sns envy and self compassion  social network service  sns  is used as a contemporary marketing tool for brands and companies  it also influences the body image of female sns users  the survey used an online survey company for female sns users in their   s and   s to analyze the effect of sns usage  the results showed that sns usage significantly impacted the internalization of a thin body  body surveillance and appearance upper comparison  the internalization of thin body also had a significant impact on body surveillance and appearance upper comparison  appearance upper comparison positively affected body dissatisfaction and eating disorders  finally  the group with higher sns envy showed higher sns usage  internalization of thin body  body surveillance  appearance upper comparison  body dissatisfaction and eating disorders  the group with higher self compassion showed opposite results  this study provided a theoretical expansion for a sns and female body image study with objectification theory and social comparison theory  it also suggests positive sns marketing strategies use for brands  lastly  this study emphasized the importance of the proper use of sns to protect the body image of sns users\nannual electric power consumption  epc  measures the electrification rate of a region  in the present study  annual composites of temporal defense meteorological satellite program operational line scan system night time light  ntl  imagery is used as a proxy measure for estimating electrification of india  a regression model is obtained using temporal ntl and epc data of the region for measuring epc from ntl data  the model is validated by using ntl data of india aquired during      and is observed to be     accurate in measuring epc from ntl data  the ntl data is also used to estimate population of the region and are found to be positively correlated with each other  the present study supports using the ntl data as a proxy measure of human wellbeing\nthe proposed ai processor architecture has high throughput for accelerating the neural network and reduces the external memory bandwidth required for processing the neural network  for achieving high throughput  the proposed super thread core  stc  includes           nano cores operating at the clock frequency of     ghz  the function safe architecture is proposed for a fault tolerance system such as an electronics system for autonomous cars  the general purpose processor  gpp  core is integrated with stc for controlling the stc and processing the ai algorithm  it has a self recovering cache and dynamic lockstep function  the function safe design has proved the fault performance has asil d of iso      standard fault tolerance levels  therefore  the entire ai processor is fabricated via the    nm cmos process as a prototype chip  its peak computing performance is    tflops at     ghz with the supply voltage of     v  the measured energy efficiency is     tops w  a gpp for control with a function safe design can have iso      asil d with the single point fault tolerance rate of\nmajor compositions of water soluble ionic species in particulate matter less than    and      m in diameter  pm   and pm     respectively  over the yellow sea were collected during the korea united states air quality  korus aq  campaign in      onboard the research vessel gisang    the secondary ionic species  nh     nss so      and no     in pm   and pm    accounted for     and     of the total analyzed species  nh    was strongly correlated with non sea salt  nss  so      nss so      in pm   and pm     no    was closely correlated with na   mg    and nss ca   in pm   and nh    in pm     high mass concentrations of methane sulfonic acid  msa  ch so      the main source of natural sulfates over the yellow sea  were observed  the concentrations of msawere found to show an increasing trend over the yellow sea in recent years  biogenic sulfur contributions to the total nss so      msa nss so     ratio  over the yellow sea ranged from      to      inpm   and from      to      inpm    during the cruise  thus  biogenic nss so     must be included  especially in the spring and early summer seasons  when biological activities are elevated in northeast asia  we classified the high aerosol mass concentration cases such as asian dust and haze cases  in asian dust cases  the ratio of no    to nss so     in the aerosols showed that mobile  stationary  sources mainly affected pm    pm      the major chemical species for asian dust cases over the yellow sea were caco   ca no     mg no     na no     and sea salt  in haze cases over the yellow sea  the contributions from stationary sources are high and the major species were  nh   so  and nh no  in pm   and pm     respectively\n"
    }
   ],
   "source": [
    "for text in corpus:\n",
    "    print(text)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       input       label\n500  session          ef\n501        e  webelement\n502        e     session\n503        e          ef\n504        e          bb\n..       ...         ...\n695       bc          ee\n696       bc           f\n697       bc           d\n698       bc          fa\n699        d           f\n\n[200 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>500</th>\n      <td>session</td>\n      <td>ef</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>e</td>\n      <td>webelement</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>e</td>\n      <td>session</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>e</td>\n      <td>ef</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>e</td>\n      <td>bb</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>695</th>\n      <td>bc</td>\n      <td>ee</td>\n    </tr>\n    <tr>\n      <th>696</th>\n      <td>bc</td>\n      <td>f</td>\n    </tr>\n    <tr>\n      <th>697</th>\n      <td>bc</td>\n      <td>d</td>\n    </tr>\n    <tr>\n      <th>698</th>\n      <td>bc</td>\n      <td>fa</td>\n    </tr>\n    <tr>\n      <th>699</th>\n      <td>d</td>\n      <td>f</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "df.iloc[500:700, :]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitvenvvenvd496da48ae144fa9baf782505b2a856e",
   "display_name": "Python 3.6.8 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}